{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andrerizzo/complete_project/blob/code/Modelagem_with_Bayesian_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yzw72g-ICLeg"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXE36bzGCLew",
    "outputId": "c0df950a-f8d1-4517-d8b1-ba6285305d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_width  petal_length_trans\n",
      "0            3.0           -1.222611\n",
      "1            3.2           -1.641609\n",
      "2            3.1           -0.860957\n",
      "3            3.6           -1.222611\n",
      "4            3.9           -0.518618\n",
      "..           ...                 ...\n",
      "144          3.0            0.767216\n",
      "145          2.5            0.537961\n",
      "146          3.0            0.767216\n",
      "147          3.4            0.860957\n",
      "148          3.0            0.658733\n",
      "\n",
      "[149 rows x 2 columns]\n",
      "              class\n",
      "0       Iris-setosa\n",
      "1       Iris-setosa\n",
      "2       Iris-setosa\n",
      "3       Iris-setosa\n",
      "4       Iris-setosa\n",
      "..              ...\n",
      "144  Iris-virginica\n",
      "145  Iris-virginica\n",
      "146  Iris-virginica\n",
      "147  Iris-virginica\n",
      "148  Iris-virginica\n",
      "\n",
      "[149 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "url1 = 'https://raw.githubusercontent.com/andrerizzo/complete_project/dataset/iris_predictors.csv'\n",
    "url2 = 'https://raw.githubusercontent.com/andrerizzo/complete_project/dataset/iris_response.csv'\n",
    "X = pd.read_csv(url1)\n",
    "y = pd.read_csv(url2)\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (16, 0)\t1.0\n",
      "  (17, 0)\t1.0\n",
      "  (18, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 0)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (23, 0)\t1.0\n",
      "  (24, 0)\t1.0\n",
      "  :\t:\n",
      "  (124, 2)\t1.0\n",
      "  (125, 2)\t1.0\n",
      "  (126, 2)\t1.0\n",
      "  (127, 2)\t1.0\n",
      "  (128, 2)\t1.0\n",
      "  (129, 2)\t1.0\n",
      "  (130, 2)\t1.0\n",
      "  (131, 2)\t1.0\n",
      "  (132, 2)\t1.0\n",
      "  (133, 2)\t1.0\n",
      "  (134, 2)\t1.0\n",
      "  (135, 2)\t1.0\n",
      "  (136, 2)\t1.0\n",
      "  (137, 2)\t1.0\n",
      "  (138, 2)\t1.0\n",
      "  (139, 2)\t1.0\n",
      "  (140, 2)\t1.0\n",
      "  (141, 2)\t1.0\n",
      "  (142, 2)\t1.0\n",
      "  (143, 2)\t1.0\n",
      "  (144, 2)\t1.0\n",
      "  (145, 2)\t1.0\n",
      "  (146, 2)\t1.0\n",
      "  (147, 2)\t1.0\n",
      "  (148, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Handling Categorical Variables\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "iris_class_encoded = encoder.fit_transform(y.values.ravel())\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "hot_encoder = OneHotEncoder()\n",
    "y_hot_encoded = hot_encoder.fit_transform(iris_class_encoded.reshape(-1,1))\n",
    "#y = y.toarray()\n",
    "print(y_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3S2bLFDMCLe0",
    "outputId": "e76e0e5b-475e-441e-9f2c-131a860cb344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_width  petal_length_trans\n",
      "27           3.4           -1.222611\n",
      "97           2.5           -0.433806\n",
      "96           2.9           -0.025237\n",
      "69           3.2            0.369964\n",
      "18           3.8           -0.860957\n",
      "..           ...                 ...\n",
      "9            3.7           -0.860957\n",
      "103          3.0            1.297001\n",
      "67           2.2            0.126509\n",
      "117          2.6            2.710835\n",
      "47           3.7           -0.860957\n",
      "\n",
      "[119 rows x 2 columns]\n",
      "     sepal_width  petal_length_trans\n",
      "133          2.6            1.046561\n",
      "109          3.2            0.658733\n",
      "59           2.0           -0.369964\n",
      "80           2.4           -0.325284\n",
      "7            2.9           -1.222611\n",
      "104          3.0            1.986482\n",
      "140          3.1            0.658733\n",
      "95           2.9           -0.075776\n",
      "118          2.2            0.537961\n",
      "84           3.4            0.126509\n",
      "33           3.1           -0.860957\n",
      "44           3.0           -1.222611\n",
      "54           2.8            0.126509\n",
      "24           3.0           -0.627699\n",
      "37           3.0           -1.641609\n",
      "132          2.8            0.658733\n",
      "111          3.0            0.923581\n",
      "73           2.9           -0.025237\n",
      "16           3.5           -1.222611\n",
      "45           3.8           -0.627699\n",
      "40           2.3           -1.641609\n",
      "8            3.1           -0.860957\n",
      "85           3.1            0.290005\n",
      "22           3.3           -0.518618\n",
      "62           2.9            0.290005\n",
      "94           3.0           -0.075776\n",
      "90           3.0            0.220472\n",
      "26           3.5           -0.860957\n",
      "43           3.8           -0.461702\n",
      "134          3.0            1.641609\n",
      "               class\n",
      "27       Iris-setosa\n",
      "97   Iris-versicolor\n",
      "96   Iris-versicolor\n",
      "69   Iris-versicolor\n",
      "18       Iris-setosa\n",
      "..               ...\n",
      "9        Iris-setosa\n",
      "103   Iris-virginica\n",
      "67   Iris-versicolor\n",
      "117   Iris-virginica\n",
      "47       Iris-setosa\n",
      "\n",
      "[119 rows x 1 columns]\n",
      "               class\n",
      "133   Iris-virginica\n",
      "109   Iris-virginica\n",
      "59   Iris-versicolor\n",
      "80   Iris-versicolor\n",
      "7        Iris-setosa\n",
      "104   Iris-virginica\n",
      "140   Iris-virginica\n",
      "95   Iris-versicolor\n",
      "118   Iris-virginica\n",
      "84   Iris-versicolor\n",
      "33       Iris-setosa\n",
      "44       Iris-setosa\n",
      "54   Iris-versicolor\n",
      "24       Iris-setosa\n",
      "37       Iris-setosa\n",
      "132   Iris-virginica\n",
      "111   Iris-virginica\n",
      "73   Iris-versicolor\n",
      "16       Iris-setosa\n",
      "45       Iris-setosa\n",
      "40       Iris-setosa\n",
      "8        Iris-setosa\n",
      "85   Iris-versicolor\n",
      "22       Iris-setosa\n",
      "62   Iris-versicolor\n",
      "94   Iris-versicolor\n",
      "90   Iris-versicolor\n",
      "26       Iris-setosa\n",
      "43       Iris-setosa\n",
      "134   Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store accuracy results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZnE6YwdCLe3",
    "outputId": "becdd51e-9587-4399-b3ab-4ea94e5d8d01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis model accuracy: 0.913 (0.095) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - LDA\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Define model\n",
    "classifier = LDA()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Linear Discriminant Analysis model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['LDA'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1qHIqZjCLe4",
    "outputId": "4164350a-7c82-4f5c-afba-8872099be9f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-Nearest Neighbors model accuracy: 0.935 (0.064) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - kNN\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define model\n",
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores= 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"k-Nearest Neighbors model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['kNN'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9DMFLfnxCLe6",
    "outputId": "c702fb3b-6fe2-4cc0-909b-3401e0b02f47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier model accuracy: 0.953 (0.055) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - SVC\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define model\n",
    "classifier = SVC(kernel='linear')\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Support Vector Classifier model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['SVC'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eTaJSCNaCLe7",
    "outputId": "bd237879-7b39-4959-cd8d-7a3b4473f90b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descend model accuracy: 0.877 (0.097) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - SGD\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Define model\n",
    "classifier = SGDClassifier()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Stochastic Gradient Descend model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['SGD'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZpjsn29CLe9",
    "outputId": "61c7ed26-cb05-4815-f190-abe117f7697e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model accuracy: 0.924 (0.054) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - Gaussian Naive Bayes\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define model\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Naive Bayes model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['Gaussian Naive Bayes'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6sWd26iaCLe9",
    "outputId": "f1b9b275-faab-44b1-e814-183f22ddc154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier model accuracy: 0.917 (0.071) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - Decision Tree\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Define model\n",
    "classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Decision Tree Classifier model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['Decision Trees'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOdavQriCLe_",
    "outputId": "7e731c7d-846b-41a1-ba43-31cc5f448988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forerst model accuracy: 0.944 (0.055) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - Random Forest\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define model\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "\n",
    "print(\"Random Forerst model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['Random Forest'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_Vxj478CLfB",
    "outputId": "a8124e66-4e51-4f43-cd10-0a939b879735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost model accuracy: 0.754 (0.111) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - AdaBoost\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define model\n",
    "classifier = AdaBoostClassifier()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')\n",
    "\n",
    "print(\"AdaBoost model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['AdaBoost'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4ltPXRcCLfC",
    "outputId": "6e109a03-f86e-4c9e-8dbe-d49e4071c7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron Model accuracy: 0.946 (0.059) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Models - MLP Classifier\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define model\n",
    "classifier = MLPClassifier()\n",
    "\n",
    "# Define model evaluation method\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Evaluate model\n",
    "scores = 0\n",
    "scores = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Multi-Layer Perceptron Model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)), \"\\n\")\n",
    "\n",
    "# Update dictionary from accuracy results\n",
    "# results = {}\n",
    "results['MLP Classifier'] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cDMF8j5CLfD",
    "outputId": "a0508d60-71ea-4c34-9b16-22f533eb0342"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SVC', 0.9530158730158731),\n",
       " ('MLP Classifier', 0.9461904761904764),\n",
       " ('Random Forest', 0.943968253968254),\n",
       " ('kNN', 0.9349206349206349),\n",
       " ('Gaussian Naive Bayes', 0.9236507936507937),\n",
       " ('Decision Trees', 0.916984126984127),\n",
       " ('LDA', 0.9133838383838384),\n",
       " ('SGD', 0.8766666666666668),\n",
       " ('AdaBoost', 0.7542857142857144)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "results = sorted(results.items(), reverse=True, key=lambda item: item[1])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZlHBfSBUT4La"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fail_fast='raise' detected. Be careful when using this mode as resources (such as Ray processes, file descriptors, and temporary files) may not be cleaned up properly. To use a safer mode, use fail_fast=True.\n",
      "Log sync requires rsync to be installed.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'C': 4.333381003524921, 'kernel': 'poly', 'degree': 1, 'gamma': 0.23132593197812873, 'coef0': 0.055529549019350644, 'shrinking': True, 'tol': 0.0019692598741096906, 'decision_function_shape': 'ovo'} \n",
      "\n",
      "SVC model accuracy after best hyperparameter definition: 0.942\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - SVC (using tune_sklearn)\n",
    "\n",
    "# Import libraries\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define search space\n",
    "hparams = dict()\n",
    "hparams['C'] = (1e-6, 100.0, \"log-uniform\")\n",
    "hparams['kernel'] = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "hparams['degree'] = (1,5, \"uniform\")\n",
    "hparams['gamma'] = (1e-6, 100.0, \"log-uniform\")\n",
    "hparams['coef0'] = (1e-6, 100.0, \"log-uniform\")\n",
    "hparams['shrinking'] = [True, False]\n",
    "hparams['tol'] = (1e-6, 100.0, \"log-uniform\")\n",
    "hparams['decision_function_shape'] = ['ovo', 'ovr']\n",
    "\n",
    "# Define the search\n",
    "tune_search = TuneSearchCV(SVC(),\n",
    "                           param_distributions = hparams,\n",
    "                           use_gpu = False,\n",
    "                           cv = 10,\n",
    "                           n_jobs = 1,\n",
    "                           search_optimization = 'bayesian', \n",
    "                           random_state = 1                                                        \n",
    "                          )\n",
    "                      \n",
    "# Perform the search\n",
    "tune_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Store result\n",
    "SVC_hyper_result = tune_search.best_params_\n",
    "\n",
    "# Report best result\n",
    "print(\"Best hyperparameters:\")\n",
    "print(SVC_hyper_result, \"\\n\")\n",
    "print(\"SVC model accuracy after best hyperparameter definition: %.3f\" % (tune_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre.rizzo\\Anaconda3\\lib\\site-packages\\skopt\\space\\space.py:502: RuntimeWarning: divide by zero encountered in log10\n",
      "  np.log10(self.low) / self.log_base,\n",
      "C:\\Users\\andre.rizzo\\Anaconda3\\lib\\site-packages\\skopt\\space\\space.py:504: RuntimeWarning: divide by zero encountered in log10\n",
      "  np.log10(self.low) / self.log_base)\n",
      "fail_fast='raise' detected. Be careful when using this mode as resources (such as Ray processes, file descriptors, and temporary files) may not be cleaned up properly. To use a safer mode, use fail_fast=True.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "The `start_trial` operation took 17.556944131851196 seconds to complete, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "The `start_trial` operation took 4.143283367156982 seconds to complete, which may be a performance bottleneck.\n",
      "The `start_trial` operation took 38.26698851585388 seconds to complete, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.\n",
      "The `start_trial` operation took 21.62541365623474 seconds to complete, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "The `start_trial` operation took 20.924090147018433 seconds to complete, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "The `start_trial` operation took 6.876642465591431 seconds to complete, which may be a performance bottleneck.\n",
      "The `start_trial` operation took 14.565850496292114 seconds to complete, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "The `start_trial` operation took 3.450493097305298 seconds to complete, which may be a performance bottleneck.\n",
      "The `process_trial` operation took 0.9980509281158447 seconds to complete, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'n_estimators': 961, 'criterion': 'entropy', 'max_depth': 99, 'min_samples_split': 4.329215461960729e-10, 'min_samples_leaf': 0.0002358407999271259, 'min_weight_fraction_leaf': 0, 'max_features': 'auto', 'max_leaf_nodes': 7, 'min_impurity_decrease': 2.6047367391637478e-05, 'oob_score': False, 'class_weight': 'balanced_subsample', 'ccp_alpha': 0.02204116122858825} \n",
      "\n",
      "Random Forest model accuracy after best hyperparameter definition: 0.925\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - Random Forest (using tune_sklearn)\n",
    "\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "\n",
    "# define search space\n",
    "hparams = dict()\n",
    "hparams['n_estimators'] = (1, 1000, 'uniform')\n",
    "hparams['criterion'] = ['gini', 'entropy']\n",
    "hparams['max_depth'] = (1,100, \"uniform\")\n",
    "hparams['min_samples_split'] = (1e-10, 9e-10, \"log-uniform\")\n",
    "hparams['min_samples_leaf'] = (1e-6, 0.5, \"log-uniform\")\n",
    "hparams['min_weight_fraction_leaf'] = (0, 0.5, \"log-uniform\")\n",
    "hparams['max_features'] = ['auto', 'sqrt', 'log2']\n",
    "hparams['max_leaf_nodes'] = (1,100, \"uniform\")\n",
    "hparams['min_impurity_decrease'] = (1e-6, 1e10, \"log-uniform\")\n",
    "#hparams['bootstrap'] = (True, False)\n",
    "hparams['oob_score'] = (True, False)\n",
    "#hparams['n_jobs'] = (-1, 1)\n",
    "hparams['class_weight'] = ('balanced', 'balanced_subsample')\n",
    "hparams['ccp_alpha'] = (1e-6, 1e10, \"log-uniform\")\n",
    "\n",
    "\n",
    "# Define evaluation\n",
    "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Define the search\n",
    "tune_search = TuneSearchCV(RFC(),\n",
    "                           param_distributions = hparams,\n",
    "                           use_gpu = False,\n",
    "                           cv = 10,\n",
    "                           n_jobs = 1,\n",
    "                           search_optimization = 'bayesian',\n",
    "                           refit = True,\n",
    "                           verbose=0,\n",
    "                           random_state=1\n",
    "                          )\n",
    "                      \n",
    "# Perform the search\n",
    "tune_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Report best result\n",
    "print(\"Best hyperparameters:\")\n",
    "print(tune_search.best_params_, \"\\n\")\n",
    "print(\"Random Forest model accuracy after best hyperparameter definition: %.3f\" % (tune_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fail_fast='raise' detected. Be careful when using this mode as resources (such as Ray processes, file descriptors, and temporary files) may not be cleaned up properly. To use a safer mode, use fail_fast=True.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n",
      "The `process_trial` operation took 0.6276142597198486 seconds to complete, which may be a performance bottleneck.\n",
      "Trial Runner checkpointing failed: cannot pickle 'dict_values' object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lsqr', 'shrinkage': None, 'tol': 2.2145813425229885}\n",
      "Multi-Layer Perceptron Model accuracy: 0.946 (0.059)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning - LDA (using tune_sklearn)\n",
    "\n",
    "from tune_sklearn import TuneSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "# define search space\n",
    "hparams = dict()\n",
    "hparams['solver'] = ['lsqr', 'eigen']\n",
    "hparams['shrinkage'] = (None,'auto')\n",
    "#hparams['n_components'] = (1, 2)\n",
    "hparams['tol'] = (1e-6, 1e10, \"log-uniform\")\n",
    "\n",
    "\n",
    "# Define evaluation\n",
    "#cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# Define the search\n",
    "tune_search = TuneSearchCV(LDA(),\n",
    "                           param_distributions = hparams,\n",
    "                           use_gpu = False,\n",
    "                           cv = 10,\n",
    "                           n_jobs = 1,\n",
    "                           search_optimization = 'bayesian',\n",
    "                           refit = True,\n",
    "                           verbose=0, \n",
    "                           random_state=1\n",
    "                          )\n",
    "                      \n",
    "# Perform the search\n",
    "tune_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Report the best result\n",
    "print(tune_search.best_params_)\n",
    "\n",
    "print(\"Multi-Layer Perceptron Model accuracy: %.3f (%.3f)\" % (np.mean(scores),np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[12  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  0  8]] \n",
      "\n",
      "Accuracy score: 1.0 \n",
      "\n",
      "Class weights\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Train SVC model using computed hyperparameters\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.metrics as clf_metrics\n",
    "\n",
    "\n",
    "# Defining model with hyperparameters\n",
    "model = SVC(C = SVC_hyper_result[\"C\"], \n",
    "            kernel = SVC_hyper_result[\"kernel\"],\n",
    "            degree = SVC_hyper_result[\"degree\"],\n",
    "            gamma = SVC_hyper_result[\"gamma\"],\n",
    "            coef0 = SVC_hyper_result[\"coef0\"],\n",
    "            shrinking = SVC_hyper_result[\"shrinking\"], \n",
    "            tol = SVC_hyper_result[\"tol\"], \n",
    "            decision_function_shape = SVC_hyper_result[\"decision_function_shape\"]\n",
    "           )\n",
    "\n",
    "# Fitting model to train set\n",
    "model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predicting with testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#clf_metrics.auc(y_test, y_pred)\n",
    "\n",
    "# Show metrics\n",
    "print('Confusion matrix')\n",
    "print(clf_metrics.confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "print('Accuracy score:', clf_metrics.accuracy_score(y_test, y_pred),\"\\n\")\n",
    "print('Class weights')\n",
    "print(model.class_weight_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train kNN model using computed hyperparameters\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Modelagem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
